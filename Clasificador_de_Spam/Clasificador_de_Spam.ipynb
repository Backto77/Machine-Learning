{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clasificador de Spam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Backto77/Machine-Learning/blob/master/Clasificador_de_Spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KCLzo_DYrNw",
        "colab_type": "text"
      },
      "source": [
        "# **1. Instalando e Importando Dependencias**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ncjl13SXpz1",
        "colab_type": "code",
        "outputId": "b727e2c7-b2be-4401-d1dc-909d3b70fa53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!pip3 install keras sklearn tqdm numpy keras_metrics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.5)\n",
            "Collecting keras_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.13.2)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DyhmwpjX8ua",
        "colab_type": "code",
        "outputId": "99460448-dc2c-4b1e-ea8a-88407a22a95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "import keras_metrics # for recall and precision metrics\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F38DgXRJZFqZ",
        "colab_type": "text"
      },
      "source": [
        "**Definamos algunos hiperparámetros:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl15N-qIYB33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LENGTH = 100 # la longitud de todas las secuencias (numero de palabras por ejemplo)\n",
        "EMBEDDING_SIZE = 100  # Uso de vectores de incrustación GloVe de 100 dimensiones\n",
        "TEST_SIZE = 0.25 # relación del conjunto de prueba\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20 # numero de Epochs\n",
        "\n",
        "# to convert labels to integers and vice-versa\n",
        "label2int = {\"ham\": 0, \"spam\": 1}\n",
        "int2label = {0: \"ham\", 1: \"spam\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE8tlUVJZYVP",
        "colab_type": "text"
      },
      "source": [
        "# **2. Cargando el Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q5VTGdmZpvg",
        "colab_type": "text"
      },
      "source": [
        "**Vamos a usar este database: https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip y la pondremos en una carpeta llamada \"data\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjNTs2FBk6SR",
        "colab_type": "code",
        "outputId": "055e4091-c977-4d2d-d3ca-eaf78c5818ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-24 04:43:12--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203415 (199K) [application/x-httpd-php]\n",
            "Saving to: ‘smsspamcollection.zip’\n",
            "\n",
            "\rsmsspamcollection.z   0%[                    ]       0  --.-KB/s               \rsmsspamcollection.z 100%[===================>] 198.65K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-09-24 04:43:12 (1.65 MB/s) - ‘smsspamcollection.zip’ saved [203415/203415]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Jwu1yNk-Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('smsspamcollection.zip', 'r') as zf:\n",
        "    zf.extractall('data/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhN3GsK_aJno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads SMS Spam Collection dataset\n",
        "    \"\"\"\n",
        "    texts, labels = [], []\n",
        "    with open(\"data/SMSSpamCollection\") as f:\n",
        "        for line in f:\n",
        "            split = line.split()\n",
        "            labels.append(split[0].strip())\n",
        "            texts.append(' '.join(split[1:]).strip())\n",
        "    return texts, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxI7rSSiaXgx",
        "colab_type": "text"
      },
      "source": [
        "**Llamamos a la funcion:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GbDY_SaadjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cargamos los datos\n",
        "X, y = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1iVLpUzaj1P",
        "colab_type": "text"
      },
      "source": [
        "# **3. Preparando el Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjiqWNcKapLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenización de texto\n",
        "# vectorizando texto, convirtiendo cada texto en secuencia de enteros\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "# convertir a secuencia de enteros\n",
        "X = tokenizer.texts_to_sequences(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rDhVNgQbFIX",
        "colab_type": "text"
      },
      "source": [
        "**Vamos a imprimir el primer ejemplo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6bix94YbKW6",
        "colab_type": "code",
        "outputId": "0bb62c61-9d09-4187-f348-b4b756aeb7cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[49, 471, 4435, 842, 755, 658, 64, 8, 1327, 88, 123, 351, 1328, 148, 2996, 1329, 67, 58, 4436, 144]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwRMwLETbP5X",
        "colab_type": "text"
      },
      "source": [
        "**Un grupo de números, cada número entero corresponde a una palabra en el vocabulario, que es lo que la red neuronal necesita de todos modos. Sin embargo, las muestras no tienen la misma longitud, necesitamos una forma de tener una secuencia de longitud fija**\n",
        "\n",
        "**Como resultado, estamos usando la función *keras.preprocessing.sequence.pad_sequences()* que rellena las secuencias al comienzo de cada secuencia con ceros:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rttYh-A-bj2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convertir numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# secuencias de pad al comienzo de cada secuencia con ceros\n",
        "# por ejemplo si SEQUENCE_LENGTH=4:\n",
        "# [[5, 3, 2], [5, 1, 2, 3], [3, 4]]\n",
        "# será transformado a:\n",
        "# [[0, 5, 3, 2], [5, 1, 2, 3], [0, 0, 3, 4]]\n",
        "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqhxSZvvcOXJ",
        "colab_type": "text"
      },
      "source": [
        "**Como recordarán, establecemos SEQUENCE_LENGTH en 100, de esta manera, todas las secuencias tienen una longitud de 100.**\n",
        "\n",
        "**Ahora nuestras etiquetas también son texto, pero vamos a hacer un enfoque diferente aquí, ya que las etiquetas son solo \"spam\" y \"ham\", necesitamos hacer one-hot encode:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNU4ylpTcmzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Etiquetas One Hot encoding\n",
        "# [spam, ham, spam, ham, ham] se convertirá a:\n",
        "# [1, 0, 1, 0, 1] y luego a:\n",
        "# [[0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]\n",
        "\n",
        "y = [ label2int[label] for label in y ]\n",
        "y = to_categorical(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2U_5fx5dQZ1",
        "colab_type": "text"
      },
      "source": [
        "**Usamos *keras.utils.to_categorial()* aquí, que hace lo que su nombre sugiere, intentemos imprimir la primera muestra de las etiquetas:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jLZUpJRc2Aa",
        "colab_type": "code",
        "outputId": "2260094f-9b41-4150-a208-5a6f06dda20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsUz3svxdn__",
        "colab_type": "text"
      },
      "source": [
        "**Eso significa que la primera muestra es \"ham\".**\n",
        "\n",
        "**A continuación, barajemos y dividamos los datos de entrenamiento y prueba:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJDKcHaZdb6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dividir y barajar\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRrQ0djmd8xx",
        "colab_type": "text"
      },
      "source": [
        "# **4. Construyendo el Modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpSNRAtZeLaA",
        "colab_type": "text"
      },
      "source": [
        "**Comencemos escribiendo una función para cargar los vectores de incrustación previamente entrenados:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzYlF2v7eAmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_vectors(tokenizer, dim=100):\n",
        "    embedding_index = {}\n",
        "    with open(f\"data/glove.6B.{dim}d.txt\", encoding='utf8') as f:\n",
        "        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vectors = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = vectors\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found will be 0s\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLcbxUMee5MA",
        "colab_type": "code",
        "outputId": "e4ddd136-1664-45cc-e53d-1ea6b3d207ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-24 04:16:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-09-24 04:16:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-09-24 04:16:39--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  55.4MB/s    in 14s     \n",
            "\n",
            "2019-09-24 04:16:52 (60.0 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIYkNPXbe8xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('glove.6B.zip', 'r') as zf:\n",
        "    zf.extractall('data/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7v9A4JZfY75",
        "colab_type": "text"
      },
      "source": [
        "**Definamos la función que construye el modelo:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjbq523EfPqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(tokenizer, lstm_units):\n",
        "    \"\"\"\n",
        "    Constructs the model,\n",
        "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
        "    \"\"\"\n",
        "    # obtener los vectores de incrustación GloVe\n",
        "    embedding_matrix = get_embedding_vectors(tokenizer)\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(tokenizer.word_index)+1,\n",
        "              EMBEDDING_SIZE,\n",
        "              weights=[embedding_matrix],\n",
        "              trainable=False,\n",
        "              input_length=SEQUENCE_LENGTH))\n",
        "\n",
        "    model.add(LSTM(lstm_units, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(2, activation=\"softmax\"))\n",
        "    # compilar como rmsprop optimizer\n",
        "    # así como con la métrica de recuerdo\n",
        "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzA6LFB2f9Rg",
        "colab_type": "text"
      },
      "source": [
        "**Tenga en cuenta que la precisión no es suficiente para determinar si el modelo está funcionando bien, eso es porque este conjunto de datos no está equilibrado, solo unas pocas muestras son spam (porque es raro). Como resultado, utilizaremos métricas de precisión y recuperación.**\n",
        "\n",
        "**Llamemos a la función:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHEwo_ROf4Kx",
        "colab_type": "code",
        "outputId": "f770f89e-034a-4ef5-bf46-e40ab887ee67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# construye el modelo con 128 unidades LSTM\n",
        "model = get_model(tokenizer=tokenizer, lstm_units=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading GloVe: 400000it [00:12, 31641.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          901000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 1,018,506\n",
            "Trainable params: 117,506\n",
            "Non-trainable params: 901,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFlxA1TNgO54",
        "colab_type": "text"
      },
      "source": [
        "# **5. Entrenando el Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zneJXk3hgH6M",
        "colab_type": "code",
        "outputId": "434f2a59-205c-47e6-ecb9-062047e727f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Inicializar nuestras devoluciones de llamada ModelCheckpoint y TensorBoard\n",
        "# model checkpoint para guardar los mejores weights\n",
        "model_checkpoint = ModelCheckpoint(\"logs/spam_classifier_{val_loss:.2f}\", save_best_only=True,\n",
        "                                    verbose=1)\n",
        "# for better visualization\n",
        "tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n",
        "# print our data shapes\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "          callbacks=[tensorboard, model_checkpoint],\n",
        "          verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape: (4180, 100)\n",
            "X_test.shape: (1394, 100)\n",
            "y_train.shape: (4180, 2)\n",
            "y_test.shape: (1394, 2)\n",
            "Train on 4180 samples, validate on 1394 samples\n",
            "Epoch 1/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0731 - acc: 0.9797 - precision: 0.9842 - recall: 0.9926 - val_loss: 0.0823 - val_acc: 0.9706 - val_precision: 0.9857 - val_recall: 0.9800\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.08234, saving model to logs/spam_classifier_0.08\n",
            "Epoch 2/20\n",
            "4180/4180 [==============================] - 17s 4ms/step - loss: 0.0630 - acc: 0.9804 - precision: 0.9868 - recall: 0.9906 - val_loss: 0.0754 - val_acc: 0.9706 - val_precision: 0.9730 - val_recall: 0.9933\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.08234 to 0.07535, saving model to logs/spam_classifier_0.08\n",
            "Epoch 3/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0519 - acc: 0.9849 - precision: 0.9877 - recall: 0.9950 - val_loss: 0.0686 - val_acc: 0.9770 - val_precision: 0.9794 - val_recall: 0.9942\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.07535 to 0.06858, saving model to logs/spam_classifier_0.07\n",
            "Epoch 4/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0464 - acc: 0.9847 - precision: 0.9882 - recall: 0.9942 - val_loss: 0.0773 - val_acc: 0.9684 - val_precision: 0.9683 - val_recall: 0.9958\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.06858\n",
            "Epoch 5/20\n",
            "4180/4180 [==============================] - 17s 4ms/step - loss: 0.0419 - acc: 0.9868 - precision: 0.9891 - recall: 0.9959 - val_loss: 0.0832 - val_acc: 0.9706 - val_precision: 0.9692 - val_recall: 0.9975\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.06858\n",
            "Epoch 6/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0341 - acc: 0.9897 - precision: 0.9918 - recall: 0.9964 - val_loss: 0.0863 - val_acc: 0.9713 - val_precision: 0.9692 - val_recall: 0.9983\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.06858\n",
            "Epoch 7/20\n",
            "4180/4180 [==============================] - 17s 4ms/step - loss: 0.0331 - acc: 0.9900 - precision: 0.9915 - recall: 0.9970 - val_loss: 0.0772 - val_acc: 0.9749 - val_precision: 0.9915 - val_recall: 0.9791\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.06858\n",
            "Epoch 8/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0315 - acc: 0.9914 - precision: 0.9940 - recall: 0.9961 - val_loss: 0.0641 - val_acc: 0.9785 - val_precision: 0.9795 - val_recall: 0.9958\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.06858 to 0.06414, saving model to logs/spam_classifier_0.06\n",
            "Epoch 9/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0235 - acc: 0.9928 - precision: 0.9937 - recall: 0.9981 - val_loss: 0.0800 - val_acc: 0.9792 - val_precision: 0.9779 - val_recall: 0.9983\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.06414\n",
            "Epoch 10/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0274 - acc: 0.9938 - precision: 0.9945 - recall: 0.9983 - val_loss: 0.0635 - val_acc: 0.9799 - val_precision: 0.9899 - val_recall: 0.9866\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.06414 to 0.06355, saving model to logs/spam_classifier_0.06\n",
            "Epoch 11/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0187 - acc: 0.9940 - precision: 0.9948 - recall: 0.9983 - val_loss: 0.0862 - val_acc: 0.9735 - val_precision: 0.9825 - val_recall: 0.9866\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.06355\n",
            "Epoch 12/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0200 - acc: 0.9950 - precision: 0.9959 - recall: 0.9983 - val_loss: 0.0674 - val_acc: 0.9799 - val_precision: 0.9819 - val_recall: 0.9950\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.06355\n",
            "Epoch 13/20\n",
            "4180/4180 [==============================] - 17s 4ms/step - loss: 0.0164 - acc: 0.9945 - precision: 0.9951 - recall: 0.9986 - val_loss: 0.0649 - val_acc: 0.9806 - val_precision: 0.9933 - val_recall: 0.9841\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.06355\n",
            "Epoch 14/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0100 - acc: 0.9976 - precision: 0.9975 - recall: 0.9997 - val_loss: 0.0739 - val_acc: 0.9835 - val_precision: 0.9892 - val_recall: 0.9917\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.06355\n",
            "Epoch 15/20\n",
            "4180/4180 [==============================] - 17s 4ms/step - loss: 0.0103 - acc: 0.9962 - precision: 0.9967 - recall: 0.9989 - val_loss: 0.0864 - val_acc: 0.9849 - val_precision: 0.9868 - val_recall: 0.9958\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.06355\n",
            "Epoch 16/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0106 - acc: 0.9964 - precision: 0.9975 - recall: 0.9983 - val_loss: 0.0793 - val_acc: 0.9828 - val_precision: 0.9900 - val_recall: 0.9900\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.06355\n",
            "Epoch 17/20\n",
            "4180/4180 [==============================] - 17s 4ms/step - loss: 0.0071 - acc: 0.9976 - precision: 0.9978 - recall: 0.9994 - val_loss: 0.0738 - val_acc: 0.9849 - val_precision: 0.9900 - val_recall: 0.9925\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.06355\n",
            "Epoch 18/20\n",
            "4180/4180 [==============================] - 17s 4ms/step - loss: 0.0088 - acc: 0.9981 - precision: 0.9986 - recall: 0.9992 - val_loss: 0.0595 - val_acc: 0.9842 - val_precision: 0.9892 - val_recall: 0.9925\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.06355 to 0.05947, saving model to logs/spam_classifier_0.06\n",
            "Epoch 19/20\n",
            "4180/4180 [==============================] - 17s 4ms/step - loss: 0.0072 - acc: 0.9981 - precision: 0.9986 - recall: 0.9992 - val_loss: 0.0773 - val_acc: 0.9849 - val_precision: 0.9916 - val_recall: 0.9908\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.05947\n",
            "Epoch 20/20\n",
            "4180/4180 [==============================] - 16s 4ms/step - loss: 0.0051 - acc: 0.9978 - precision: 0.9983 - recall: 0.9992 - val_loss: 0.0915 - val_acc: 0.9878 - val_precision: 0.9909 - val_recall: 0.9950\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.05947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f135065cb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ul8O0gCiwVk",
        "colab_type": "text"
      },
      "source": [
        "# **6. Evaluando el Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "relTTwTEi0MG",
        "colab_type": "code",
        "outputId": "29287763-4192-4c13-e2d6-50685a42ba57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# obtener la pérdida y las métricas\n",
        "result = model.evaluate(X_test, y_test)\n",
        "# extraer esos\n",
        "loss = result[0]\n",
        "accuracy = result[1]\n",
        "precision = result[2]\n",
        "recall = result[3]\n",
        "\n",
        "print(f\"[+] Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"[+] Precision:   {precision*100:.2f}%\")\n",
        "print(f\"[+] Recall:   {recall*100:.2f}%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1394/1394 [==============================] - 4s 3ms/step\n",
            "[+] Accuracy: 98.78%\n",
            "[+] Precision:   99.09%\n",
            "[+] Recall:   99.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JvF7oaMjFO5",
        "colab_type": "text"
      },
      "source": [
        "Esto es lo que significa cada métrica:\n",
        "\n",
        "*   **Accuracy:** porcentaje de predicciones correctas.\n",
        "*   **Recall:** porcentaje de correos electrónicos no deseados que se predijeron correctamente.\n",
        "* **Precision:** porcentaje de correos electrónicos clasificados como spam que en realidad eran spam\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x3B8LNejqbc",
        "colab_type": "text"
      },
      "source": [
        "**¡Excelente! vamos a probar esto:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq3xrp7VjuzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    # rellenar la secuencia\n",
        "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "    # obtener prediccion\n",
        "    prediction = model.predict(sequence)[0]\n",
        "    # one-hot encoded vector, revertir usando np.argmax\n",
        "    return int2label[np.argmax(prediction)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd2llYFGkHC5",
        "colab_type": "text"
      },
      "source": [
        "**Vamos a crear un texto spam**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfG3k4IzkAyq",
        "colab_type": "code",
        "outputId": "dcf1702a-11ba-434a-c4d2-0a3286e95bf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = \"Congratulations! you have won 100,000$ this week, click here to claim fast\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLZy0bMdkSV9",
        "colab_type": "text"
      },
      "source": [
        "**OK, ahora uno legitimo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6vQWH_TkPoY",
        "colab_type": "code",
        "outputId": "3837d0b5-43e5-41b2-ce02-f84dfcabc481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = \"Hi man, I was wondering if we can meet tomorrow.\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
